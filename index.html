<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>My Project</title>
<link rel="stylesheet" href="style.css">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
<style>
.project-title {
    font-size: 2.0rem;   /* 제목 크기 */
    line-height: 1.2;    /* 줄 간격 */
}
</style>
</head>
<body>

<!-- Hero Section -->
<section class="text-center py-5">
    <h1 class="display-4 fw-bold project-title">
        SHIFT-RL: Sensor-driven Hierarchical Information Fusion Transformer<br>
        for BEV-based Maneuvering in Dense Multi-lane Environments
    </h1>

    <!-- Authors -->
    <p class="lead mt-3">
        Sungjun Heo<sup>1</sup>, 
        Jeong hwan Jeon<sup>1,2,*</sup>
    </p>
    
    <!-- Affiliations -->
    <p class="mb-4">
        <sup>1</sup>Graduate School of Artificial Intelligence, UNIST<br>
        <sup>2</sup>Department of Electrical Engineering, UNIST<br>
        <sup>*</sup> Corresponding author
    </p>

    <div class="mt-3">
        <a href="paper/paper.pdf" class="btn btn-primary">Paper(arXiv)</a>
        <a href="https://github.com/myrepo" class="btn btn-secondary">Code(Comming Soon)</a>
        <a href="https://youtu.be/45-SNFv8wkY?si=s8xZUb6_ZNx9ADYQ" class="btn btn-success">Video</a>
    </div>
</section>

<!-- Video -->
<section id="video" class="container text-center my-5">
    <h2>Video</h2>
    <div class="ratio ratio-16x9">
        <iframe src="https://youtu.be/45-SNFv8wkY?si=s8xZUb6_ZNx9ADYQ" allowfullscreen></iframe>
    </div>
</section>

<!-- Abstract -->
<section id="abstract" class="container my-5">
    <h2>Abstract</h2>
    <div class="text-center">
        <img src="assets/img/overview.png" class="img-fluid rounded" style="width: 50%;">
    </div>
    <p>Imitation learning is widely used in end‑to‑end (E2E) autonomous driving, but it is reported to be challenging when tasks require long‑horizon planning or face substantial out-of-distribution (OOD) shifts. Reinforcement learning (RL) based planning is a promising alternative, yet monolithic policies are brittle under sparse rewards and large state spaces. We introduce SHIFT‑RL (Sensor‑driven Hierarchical Information Fusion Transformer for RL), a hierarchical framework that operates on LiDAR‑based bird’s‑eye‑view (BEV) states, fuses spatiotemporal cues with an information‑fusion Transformer, and couples hierarchical policies with curriculum learning. A high‑level policy selects tactical maneuvers; a low‑level policy, conditioned on that choice, outputs vehicle speed, decomposing complex decisions into stable subproblems. We train and evaluate SHIFT‑RL in dense multi‑lane settings that require long‑horizon planning and exhibit significant OOD shifts. Ablations of the hierarchy and curriculum, and comprehensive comparisons against monolithic RL, rule‑based methods, and an imitation learning baseline show that SHIFT‑RL achieves the highest success rate and the lowest collision rate across all evaluated environments. These results indicate that combining an information‑fusion Transformer with hierarchical RL and curriculum learning substantially improves stability and generalization for challenging BEV‑based maneuvering.</p>
</section>

<!-- Pipeline -->
<section class="container my-5">
    <h2>Pipeline</h2>
    <div class="text-center">
    <img src="assets/img/pipeline.png" class="img-fluid rounded" style="width: 75%;">
    </div>
    <p>HD maps and LiDAR in the CARLA simulator are fused in real time to produce BEV images, which are encoded by an autoencoder to form temporal BEV tokens. The tokens are processed by hierarchy‑specific Transformer encoders via self‑attention and fused with embedded ego information through cross‑attention. In the hierarchical design, the high‑level module uses a Dueling DDQN to decide maneuvers, while the low‑level module, conditioned on the high‑level command and the attended features, employs SAC to output continuous controls for the ego vehicle.</p>
</section>

<!-- Result -->
<section class="container my-5">
    <h2>Pipeline</h2>
    <div class="text-center">
    <img src="assets/img/result.PNG" class="img-fluid rounded" style="width: 75%;">
    </div>
    <p>Success and collision rates (\%) across three environments—Base, High, and Extreme-High.} We evaluate ten methods grouped into six categories: the proposed SHIFT-RL; hybrid learning-plus-rule approaches; rule-based models; monolithic RL with curriculum learning; monolithic RL without curriculum learning; and imitation learning. Each method is tested for 500 episodes per environment.</p>
</section>

</body>
</html>
